{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21be91b",
   "metadata": {},
   "source": [
    "# Personalized News Aggregator\n",
    "\n",
    "###### End Gold :  Develop a news aggregator that scrapes articles from multiple sources, categorizes them, and provides access via a REST API and a simple front-end interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fcf40f",
   "metadata": {},
   "source": [
    "#### IMPORTING REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461a409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import requests  # For making HTTP requests to fetch web pages\n",
    "from datetime import datetime  # For working with date and time\n",
    "import pandas as pd  # For data manipulation and storage\n",
    "import spacy  # For NLP tasks like tokenization and lemmatization\n",
    "import re  # For regular expressions used in text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7d3f2",
   "metadata": {},
   "source": [
    "## Part 1: News Scraping\n",
    "### Objective:\n",
    "Scrape news articles from multiple sources (any 2 news sources e.g., BBC, CNN, Times of India, etc.) and collect the following data:\n",
    "\n",
    "- **Title**: The article's headline.\n",
    "- **Summary**: A brief overview or the first few sentences.\n",
    "- **Publication Date**: The date the article was published.\n",
    "- **Source**: The news outlet's name.\n",
    "- **URL**: Link to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513f1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the news page to scrape\n",
    "url = 'https://timesofindia.indiatimes.com/news'\n",
    "\n",
    "# Fetch the page content using requests\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa012db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find the specific section that contains news articles (adjust class as needed)\n",
    "news = soup.find('ul', class_ = 'HytnJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all articles within the section (adjust tag and class based on website structure)\n",
    "articles = news.find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles) # Output the number of articles found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080db807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the titles, summaries, and URLs of the articles\n",
    "titles = [article.find('p', class_ = 'CRKrj').text if article.find('p', class_ = 'CRKrj')!= None else None for article in articles]\n",
    "summary = [article.find('p', class_ = 'W4Hjm').text if article.find('p', class_ = 'W4Hjm')!= None else None for article in articles]\n",
    "urls = [article.find('a', class_ = 'VeCXM').get('href') if article.find('a', class_ = 'VeCXM')!= None else None for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bef63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date string to datetime format\n",
    "def date_time(date):\n",
    "    return datetime.strptime(date.strip(\"Update:\").strip(\"IST\").strip(), \"%b %d, %Y, %H:%M\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2cffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = []\n",
    "source = []\n",
    "# Iterate through the article URLs to scrape additional information from individual pages\n",
    "for url in urls:\n",
    "    response = requests.get(url)  # Fetch the article page\n",
    "    soup = BeautifulSoup(response.text, \"html\")  # Parse the article HTML content\n",
    "    \n",
    "    # Extract the publication date from the article page\n",
    "    date_ = soup.find(\"div\", class_ = 'xf8Pm').find('span').text\n",
    "    date.append(date_time(date_))  # Convert date to datetime format and add to list\n",
    "    \n",
    "    # Extract the source (e.g., Times of India) from the article page, handle cases where source is not available\n",
    "    source_ = soup.find(\"div\", class_ = 'kgcOp').find('a').text if soup.find(\"div\", class_ = 'kgcOp')!= None else None\n",
    "    source.append(source_)  # Add source to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea482c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the scraped data\n",
    "data = { 'Title' : titles, 'Summary' : summary, 'URL' : urls, 'Source' : source, 'Date' : date}\n",
    "df = pd.DataFrame(data) # Convert the dictionary into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53692cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('news_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3531e",
   "metadata": {},
   "source": [
    "## Part 2: Content Categorization\n",
    "\n",
    "### Objective:\n",
    "Use NLP to categorize articles into topics (e.g., politics, technology, sports).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ced30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the text by removing non-alphabetic characters, lowercasing, and lemmatizing\n",
    "def preprocess_text(text):\n",
    "    filtered_text = ' '.join(re.sub('[^a-z^A-Z]', ' ', text).lower().split())\n",
    "    doc = nlp(filtered_text)\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cda452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping categories to their relevant keywords for article classification\n",
    "category_keywords = {\n",
    "    \"World news\": [\"world\", \"international\", \"global\", \"abroad\"],\n",
    "    \"Editorial\": [\"opinion\", \"editorial\", \"column\", \"commentary\"],\n",
    "    \"Obituaries\": [\"obituary\", \"death\", \"died\", \"passed away\"],\n",
    "    \"Business\": [\"business\", \"economy\", \"finance\", \"market\", \"stock\"],\n",
    "    \"Lifestyle journalism\": [\"lifestyle\", \"fashion\", \"travel\", \"leisure\"],\n",
    "    \"Weather\": [\"weather\", \"forecast\", \"temperature\", \"climate\"],\n",
    "    \"Business journalism\": [\"corporate\", \"startup\", \"trade\", \"merger\"],\n",
    "    \"Science journalism\": [\"science\", \"research\", \"technology\", \"innovation\"],\n",
    "    \"Crime news\": [\"crime\", \"murder\", \"theft\", \"assault\", \"police\"],\n",
    "    \"Political journalism\": [\"politics\", \"government\", \"election\", \"policy\"],\n",
    "    \"Government\": [\"government\", \"policy\", \"administration\", \"regulation\"],\n",
    "    \"Local news\": [\"local\", \"community\", \"neighborhood\", \"city\", \"town\"],\n",
    "    \"Press release\": [\"press release\", \"announcement\", \"statement\"],\n",
    "    \"Feature\": [\"feature\", \"profile\", \"in-depth\", \"special report\"],\n",
    "    \"Health education\": [\"health\", \"wellness\", \"medicine\", \"fitness\"],\n",
    "    \"Sports\": [\"sports\", \"game\", \"match\", \"tournament\", \"athlete\"],\n",
    "    \"Letters to the editor\": [\"letter\", \"reader\", \"response\", \"feedback\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize articles based on keywords\n",
    "def categorize_article(text):\n",
    "    if not text:\n",
    "        return \"Uncategorized\"\n",
    "    \n",
    "    processed_text = preprocess_text(text) # Preprocess the article summary\n",
    "    \n",
    "    matched_categories = []\n",
    "    \n",
    "    # Check if any keywords match the processed tex\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in processed_text for keyword in keywords):\n",
    "            matched_categories.append(category)\n",
    "            \n",
    "    # Return the matched categories or \"Uncategorized\" if no matches\n",
    "    return \", \".join(matched_categories) if matched_categories else \"Uncategorized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the categorization function to the \"Summary\" column\n",
    "df['Category'] = df['Summary'].apply(categorize_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('news_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8df6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
